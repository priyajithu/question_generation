{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6daf47f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91999\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\91999\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  In what Python language is the self keyword used?\n",
      "Answer: In Python, the self keyword is used as a convention to refer to the instance of a class within the class itself.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91999\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\91999\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What does python do with decorators?\n",
      "Answer: Decorators in python are a way to modify or enhance the behavior of functions or classes without directly modifying their source code.\n",
      "\n",
      "Question:  What is PEP 8?\n",
      "Answer: PEP 8 is a style guide for Python code and PEP 8 specifically focuses on the style conventions for writing Python code.\n",
      "\n",
      "Question:  What does Python use private heap space for?\n",
      "Answer: Python uses private heap space to manage memory.\n",
      "\n",
      "Question:  What is inheritance?\n",
      "Answer: In Python, inheritance is a mechanism that allows a class to inherit properties (attributes and methods) from another class.\n",
      "\n",
      "Question:  What does Django provide built-in support for?\n",
      "Answer: Django provides built-in support for internationalization (i18n) and localization (l10n), it includes features for translating text, handling date and number formatting based on different locales, and switching between multiple languages in the application\n",
      "\n",
      "Question:  What is the Django admin site?\n",
      "Answer: The Django admin site is an automatically generated administration interface that allows authorized users to manage the application's data models.\n",
      "\n",
      "Question:  What version of the MVC architectural pattern does Django follow?\n",
      "Answer: Django follows a slightly modified version of the MVC architectural pattern, known as Model-View-Template (MVT). Models represent the data structure, views handle the business logic and interaction with models, and templates handle the presentation layer.\n",
      "\n",
      "Question:  What does Django middleware stand for?\n",
      "Answer: Django middleware is a component that sits between the web server and the view function, allowing you to process requests and responses globally.\n",
      "\n",
      "Question:  What is Django's template system?\n",
      "Answer: Django's template system allows developers to separate the presentation logic from the Python code.\n",
      "\n",
      "Question:  What does NumPy stand for?\n",
      "Answer: NumPy stands for Numerical Python and it is a powerful library in Python for numerical computations, Its main purpose is to provide efficient and convenient handling of large arrays and matrices of numerical data, along with a collection of mathematical functions to operate on these arrays.\n",
      "\n",
      "Question:  What is a NumPy array?\n",
      "Answer: A NumPy array is a grid of values, all of the same data type, and indexed by a tuple of non-negative integers. It is the fundamental data structure in NumPy and provides several advantages over regular Python lists, including faster execution of operations, optimized memory usage, and a wide range of mathematical functions and operations specifically designed for arrays.\n",
      "\n",
      "Question:  What is a multi-dimensional array in NumPy?\n",
      "Answer: A one-dimensional array in NumPy is a simple list of values, while a multi-dimensional array represents a table of elements with rows and columns, similar to a matrix. Multi-dimensional arrays can have any number of dimensions, such as 2D for matrices or 3D for representing volumes of data.\n",
      "\n",
      "Question:  How can you resize a NumPy array?\n",
      "Answer: You can reshape or resize a NumPy array using the np.reshape() function or by directly modifying the shape attribute of the array. For example: new_arr = np.reshape(arr, (2, 3)) or arr.shape = (2, 3)\n",
      "\n",
      "Question:  What does NumPy provide functions to generate arrays with specific patterns or values?\n",
      "Answer: NumPy provides several functions to generate arrays with specific patterns or values, such as np.zeros(), np.ones(), np.arange(), np.linspace(), np.random.rand(), np.random.randint(), and np.eye().\n",
      "\n",
      "Question:  What module does scikit-learn provide?\n",
      "Answer: sklearn provides various modules and components, including datasets for loading and exploring datasets, preprocessing for data preprocessing techniques, model_selection for model selection and evaluation, metrics for performance evaluation metrics, estimators for machine learning algorithms, and ensemble for ensemble methods, among others.\n",
      "\n",
      "Question:  What does scikit-learn do?\n",
      "Answer: Estimator objects in sklearn are used to fit models to data and make predictions. They encapsulate the learning algorithms and have methods such as fit() to train the model, predict() to make predictions, and score() to evaluate the model's performance.\n",
      "\n",
      "Question:  What is sklearn's datasets module?\n",
      "Answer: sklearn provides a datasets module that offers various datasets for practice and experimentation. You can use functions like load_iris(), load_digits(), or fetch_california_housing() to load datasets. Once loaded, you can explore the dataset attributes, such as data, target, feature_names, and target_names.\n",
      "\n",
      "Question:  What preprocessing techniques does scikit-learn offer?\n",
      "Answer: sklearn offers several preprocessing techniques, including scaling features with StandardScaler or MinMaxScaler, handling missing values with SimpleImputer, encoding categorical variables with OneHotEncoder or LabelEncoder, and reducing dimensionality with techniques like Principal Component Analysis (PCA).\n",
      "\n",
      "Question:  What is the Pipeline module in scikit-learn?\n",
      "Answer: The Pipeline module in scikit-learn allows you to chain multiple preprocessing steps and machine learning models into a single object. It helps in organizing and automating the workflow, enabling easier model building, training, and evaluation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries:\n",
    "# This code imports the T5 model and tokenizer from the transformers library. These are essential for working with the T5 model\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "\n",
    "# Load the pre-trained T5 model & tokenizer\n",
    "# Here, the code loads the pre-trained T5 model (T5ForConditionalGeneration) using the from_pretrained method. \n",
    "# The model you're using is 'ramsrigouthamg/t5_squad_v1'.\n",
    "# Similarly, the code initializes the T5 tokenizer (T5Tokenizer) using the from_pretrained method. \n",
    "# The tokenizer uses the 't5-base' model and has a maximum sequence length of 1024 tokens.\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "question_model = T5ForConditionalGeneration.from_pretrained('priya_t5_squad_v1')\n",
    "question_tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=1024)\n",
    "\n",
    "# Defining the get_question_from_answer function:\n",
    "# This function takes an answer and its corresponding context as inputs. \n",
    "# It then creates a string text that concatenates the context and answer with the necessary formatting.\n",
    "\n",
    "def get_question_from_answer(answer, context):\n",
    "    text = f\"context: {context} answer: {answer} </s>\"\n",
    "\n",
    "    # Tokenize the input text using the tokenizer\n",
    "    # The code tokenizes the text using the tokenizer's encode_plus method. \n",
    "    # It sets the max_length to 256 and enables padding and truncation. \n",
    "    # The truncation_strategy parameter determines how the text is truncated if it exceeds the max_length. The encoded input is returned as PyTorc sensors.\n",
    "    \n",
    "    max_len = 256\n",
    "    encoding = question_tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,\n",
    "        truncation_strategy=\"longest_first\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "   # Generate the question using the T5 model\n",
    "   # this code prepares the input tensors (input_ids and attention_mask) from the encoding. \n",
    "   # Then, it passes these tensors to the T5 model's generate method to generate questions. \n",
    "   # Several parameters are provided, such as num_beams (the number of beams for beam search), \n",
    "   # no_repeat_ngram_size (prevents repetition of n-grams), and max_length (maximum length of the generated question).\n",
    "    \n",
    "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "    outs = question_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        early_stopping=True,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        max_length=200\n",
    "    )\n",
    "\n",
    "    # Decode and format the question\n",
    "    # The code decodes the generated question tensors (outs) using the tokenizer's decode method. \n",
    "    # It removes the prefix \"question:\" and leading/trailing whitespace from the decoded text. \n",
    "    # The resulting question is returned by the function.\n",
    "    \n",
    "    dec = [question_tokenizer.decode(ids) for ids in outs]\n",
    "    question = dec[0].replace(\"question:\", \"\").strip()\n",
    "    return question\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "# Reading a JSON file\n",
    "# This code reads a JSON file containing interview schema data. \n",
    "# The file is opened and its contents are loaded into the data variable.\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\91999\\\\Downloads\\\\interviewerSchema.json.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "data\n",
    "\n",
    "\n",
    "# Check if Python, Django, NumPy, and scikit-learn (sklearn) skills are present in the schema\n",
    "# these lines check if the skills \"python,\" \"django,\" \"numpy,\" and \"sklearn\" are present in the JSON schema.\n",
    "# It iterates through the \"Techskills\" list in the schema and checks if the corresponding skill is present using the any() function.\n",
    "\n",
    "python_skill_present = any(skill.get(\"python\") for skill in data.get(\"skills\", {}).get(\"Techskills\", []))\n",
    "django_skill_present = any(skill.get(\"django\") for skill in data.get(\"skills\", {}).get(\"Techskills\", []))\n",
    "numpy_skill_present = any(skill.get(\"numpy\") for skill in data.get(\"skills\", {}).get(\"Techskills\", []))\n",
    "sklearn_skill_present = any(skill.get(\"sklearn\") for skill in data.get(\"skills\", {}).get(\"Techskills\", []))\n",
    "\n",
    "\n",
    "# Generate Python questions\n",
    "# Generating Python, Django, NumPy, and scikit-learn questions:\n",
    "# Based on the skills present in the schema, the code generates questions for each skill using the get_question_from_answer() function and prints the generated question along with the original answer.\n",
    "\n",
    "if python_skill_present:\n",
    "    \n",
    "    python_answers = [\n",
    "        \n",
    "        \"In Python, the self keyword is used as a convention to refer to the instance of a class within the class itself.\",\n",
    "        \"Decorators in python are a way to modify or enhance the behavior of functions or classes without directly modifying their source code.\",\n",
    "        \"PEP 8 is a style guide for Python code and PEP 8 specifically focuses on the style conventions for writing Python code.\",\n",
    "        \"Python uses private heap space to manage memory.\", \n",
    "        \"In Python, inheritance is a mechanism that allows a class to inherit properties (attributes and methods) from another class.\"\n",
    "    ]\n",
    "    \n",
    "    for answer in python_answers:\n",
    "        generated_question = get_question_from_answer(answer, \"Python\")\n",
    "        generated_question = generated_question.replace(\"<pad> \", \"\").replace(\"</s>\", \"\")\n",
    "        print(f\"Question: {generated_question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# Generate Django questions\n",
    "\n",
    "if django_skill_present:\n",
    "    django_answers = [\n",
    "        \n",
    "        \"Django provides built-in support for internationalization (i18n) and localization (l10n), it includes features for translating text, handling date and number formatting based on different locales, and switching between multiple languages in the application\",\n",
    "        \"The Django admin site is an automatically generated administration interface that allows authorized users to manage the application's data models.\",\n",
    "        \"Django follows a slightly modified version of the MVC architectural pattern, known as Model-View-Template (MVT). Models represent the data structure, views handle the business logic and interaction with models, and templates handle the presentation layer.\",\n",
    "        \"Django middleware is a component that sits between the web server and the view function, allowing you to process requests and responses globally.\",\n",
    "        \"Django's template system allows developers to separate the presentation logic from the Python code.\"\n",
    "    \n",
    "    ]\n",
    "    \n",
    "    for answer in django_answers:\n",
    "        generated_question = get_question_from_answer(answer, \"Django\")\n",
    "        generated_question = generated_question.replace(\"<pad> \", \"\").replace(\"</s>\", \"\")\n",
    "        print(f\"Question: {generated_question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# Generate NumPy questions\n",
    "\n",
    "if numpy_skill_present:  \n",
    "    numpy_answers = [\n",
    "        \n",
    "        \"NumPy stands for Numerical Python and it is a powerful library in Python for numerical computations, Its main purpose is to provide efficient and convenient handling of large arrays and matrices of numerical data, along with a collection of mathematical functions to operate on these arrays.\",\n",
    "        \"A NumPy array is a grid of values, all of the same data type, and indexed by a tuple of non-negative integers. It is the fundamental data structure in NumPy and provides several advantages over regular Python lists, including faster execution of operations, optimized memory usage, and a wide range of mathematical functions and operations specifically designed for arrays.\",\n",
    "        \"A one-dimensional array in NumPy is a simple list of values, while a multi-dimensional array represents a table of elements with rows and columns, similar to a matrix. Multi-dimensional arrays can have any number of dimensions, such as 2D for matrices or 3D for representing volumes of data.\",\n",
    "        \"You can reshape or resize a NumPy array using the np.reshape() function or by directly modifying the shape attribute of the array. For example: new_arr = np.reshape(arr, (2, 3)) or arr.shape = (2, 3)\",\n",
    "        \"NumPy provides several functions to generate arrays with specific patterns or values, such as np.zeros(), np.ones(), np.arange(), np.linspace(), np.random.rand(), np.random.randint(), and np.eye().\"\n",
    "    \n",
    "    ]\n",
    "    for answer in numpy_answers:\n",
    "        generated_question = get_question_from_answer(answer, \"NumPy\")\n",
    "        generated_question = generated_question.replace(\"<pad> \", \"\").replace(\"</s>\", \"\")\n",
    "        print(f\"Question: {generated_question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print()\n",
    "\n",
    "# Generate scikit-learn (sklearn) questions\n",
    "\n",
    "if sklearn_skill_present:\n",
    "    sklearn_answers = [\n",
    "        \n",
    "        \"sklearn provides various modules and components, including datasets for loading and exploring datasets, preprocessing for data preprocessing techniques, model_selection for model selection and evaluation, metrics for performance evaluation metrics, estimators for machine learning algorithms, and ensemble for ensemble methods, among others.\",\n",
    "        \"Estimator objects in sklearn are used to fit models to data and make predictions. They encapsulate the learning algorithms and have methods such as fit() to train the model, predict() to make predictions, and score() to evaluate the model's performance.\",\n",
    "        \"sklearn provides a datasets module that offers various datasets for practice and experimentation. You can use functions like load_iris(), load_digits(), or fetch_california_housing() to load datasets. Once loaded, you can explore the dataset attributes, such as data, target, feature_names, and target_names.\",\n",
    "        \"sklearn offers several preprocessing techniques, including scaling features with StandardScaler or MinMaxScaler, handling missing values with SimpleImputer, encoding categorical variables with OneHotEncoder or LabelEncoder, and reducing dimensionality with techniques like Principal Component Analysis (PCA).\",\n",
    "        \"The Pipeline module in scikit-learn allows you to chain multiple preprocessing steps and machine learning models into a single object. It helps in organizing and automating the workflow, enabling easier model building, training, and evaluation.\"\n",
    "    \n",
    "     ]\n",
    "    \n",
    "    for answer in sklearn_answers:\n",
    "        generated_question = get_question_from_answer(answer, \"scikit-learn\")\n",
    "        generated_question = generated_question.replace(\"<pad> \", \"\").replace(\"</s>\", \"\")\n",
    "        print(f\"Question: {generated_question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print()\n",
    "        \n",
    "# If none of the skills are present in the schema, this message is printed to indicate that no questions can be generated.\n",
    "\n",
    "else:\n",
    "    print(\"No Python, django, numpy and sklearn tech skill found in the JSON schema.\")\n",
    "\n",
    "# Overall, the code reads a JSON schema, checks for specific skills, and generates questions based on those skills using a pre-trained T5 model for question generation. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687364d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# # Save the model as a pickle file\n",
    "# output_file = \"priya_model.pkl\"\n",
    "\n",
    "# # Serialize and save the model\n",
    "# with open(output_file, \"wb\") as file:\n",
    "#     pickle.dump(question_model, file)\n",
    "\n",
    "# # Save the tokenizer separately as a pickle file\n",
    "# tokenizer_file = \"tokenizer.pkl\"\n",
    "\n",
    "# with open(tokenizer_file, \"wb\") as file:\n",
    "#     pickle.dump(question_tokenizer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45410b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "\n",
    "# In the provided code, the tokenizer is an instance of the T5Tokenizer class from the Hugging Face Transformers library. \n",
    "# The tokenizer is responsible for processing and converting raw text into numerical inputs that can be understood by the T5 model.\n",
    "# The T5Tokenizer implements the tokenization algorithm specific to the T5 model architecture. \n",
    "# It breaks down the input text into individual tokens, assigns unique numerical IDs to each token, and performs various text preprocessing steps, such as handling punctuation, special characters, and encoding/decoding the text.\n",
    "\n",
    "# Here, the tokenizer is loaded with the 't5-base' pre-trained model. \n",
    "# The from_pretrained method downloads the tokenizer's vocabulary and other necessary files associated with the specified model.\n",
    "# The encode_plus method tokenizes the given text according to the tokenizer's rules. It applies the specified parameters, such as max_length (maximum length of the tokens), pad_to_max_length (whether to pad the tokens to the specified maximum length), truncation (whether to truncate the tokens if they exceed the maximum length), and truncation_strategy (strategy for truncating tokens).\n",
    "# The tokenizer's encode_plus method returns a dictionary-like object called encoding, which contains the tokenized input in the form of input IDs and attention masks. These encoded inputs are then used as input to the T5 model for question generation.\n",
    "# Overall, the tokenizer plays a crucial role in preparing the text data for the T5 model and converting it into a suitable format for further processing and training.\n",
    "\n",
    "\n",
    "# significance of t5 model\n",
    "\n",
    "# The T5 (Text-To-Text Transfer Transformer) model is a powerful transformer-based model that has gained significance in natural language processing (NLP) tasks. It was introduced by Google Research and has achieved state-of-the-art performance on various NLP benchmarks.\n",
    "# The significance of the T5 model lies in its versatility and capability to handle a wide range of NLP tasks through a unified text-to-text framework. Unlike traditional models that are designed for specific tasks, such as machine translation or question answering, T5 can be fine-tuned and applied to a diverse set of tasks using a consistent input-output format.\n",
    "# Here are some key aspects of the T5 model's significance:\n",
    "# Text-to-Text Framework: \n",
    "# T5 introduces a unified framework where different NLP tasks can be framed as text-to-text transformations. The model is trained to map input text (source) to output text (target) using a large corpus of data. This framework allows for flexible task formulation and enables fine-tuning for specific downstream tasks.\n",
    "# Large-Scale Pre-training: \n",
    "# T5 benefits from pre-training on a massive amount of publicly available text from the internet. This extensive pre-training helps the model learn general language representations and capture rich semantic and syntactic information. It provides a strong foundation for transfer learning and improves performance on downstream tasks with limited labeled data.\n",
    "# Transfer Learning and Fine-tuning: \n",
    "# T5's pre-trained weights can be fine-tuned on specific tasks with task-specific datasets. This transfer learning approach allows the model to leverage its pre-trained knowledge to adapt and specialize for specific NLP tasks. Fine-tuning on task-specific data helps improve performance and reduces the need for extensive labeled training data.\n",
    "# Wide Range of NLP Tasks: \n",
    "# T5 can be applied to a wide array of NLP tasks such as text classification, machine translation, text summarization, question answering, sentiment analysis, and more. The model's flexibility and the text-to-text framework enable easy adaptation to different tasks by providing appropriate input-output format and fine-tuning on task-specific data.\n",
    "# State-of-the-Art Performance:\n",
    "# T5 has demonstrated state-of-the-art performance on various NLP benchmarks and competitions. \n",
    "# It has achieved top ranks in tasks like machine translation, question answering, text summarization, and others. Its performance is a testament to the power of large-scale pre-training and transfer learning in NLP.\n",
    "\n",
    "# In summary, the significance of the T5 model lies in its versatile text-to-text framework, large-scale pre-training, transfer learning capabilities, \n",
    "# and impressive performance across a range of NLP tasks. \n",
    "# It has emerged as a valuable tool for natural language understanding and generation tasks, enabling researchers and practitioners to tackle a diverse set of NLP challenges with a unified and powerful model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
